import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time
from tqdm import tqdm
from config import config
from model import ResNet18

CIFAR10_CLASSES = ('plane', 'car', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck')

def get_dataloaders():
    # CIFAR-10æ•°æ®é¢„å¤„ç† compose()ä¸²è”pipeline
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        # å½’ä¸€åŒ–[0,1]
        transforms.ToTensor(),
        # æ ‡å‡†åŒ– åŠ é€Ÿæ”¶æ•›
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    # åŠ è½½CIFAR-10æ•°æ®é›†
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)

    return train_loader, test_loader, CIFAR10_CLASSES

def train():
    # è·å–æ•°æ®
    train_loader, test_loader, class_names = get_dataloaders()
    print(f"æ•°æ®é›†ï¼šCIFAR-10")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
    print(f"æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}")
    print(f"ç±»åˆ«: {class_names}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = ResNet18(config.num_classes)
    model.to(config.device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate,
                          momentum=config.momentum, weight_decay=config.weight_decay)

    # æ¯éš”step_sizeçš„epochsï¼Œlearning rate * gamma
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)
    best_acc = 0
    train_losses = []
    test_accuracies = []

    print("\n training starts")
    for epoch in range(config.num_epochs):
        start_time = time.time()

        model.train()
        train_loss = 0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{config.num_epochs}')
        for batch_idx, (inputs, targets) in enumerate(pbar):
            #1 inputs & targetsç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            inputs, targets = inputs.to(config.device), targets.to(config.device)
            #2 æ¯ä¸ªbatchä¹‹å‰è¿›è¡Œæ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            #3 å‰å‘ä¼ æ’­
            outputs = model(inputs)
            #4 æŸå¤±å‡½æ•°
            loss = criterion(outputs, targets)
            #5 åå‘ä¼ æ’­
            loss.backward()
            #6 æ›´æ–°å‚æ•°
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({
                'loss': train_loss / (batch_idx + 1),
                'acc': 100. * correct / total
            })

        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_loss = 0
        test_correct = 0
        test_total = 0

        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(config.device), targets.to(config.device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()

        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100. * correct / total
        test_acc = 100. * test_correct / test_total
        epoch_time = time.time() - start_time

        # è®°å½•å†å²
        train_losses.append(train_loss / len(train_loader))
        test_accuracies.append(test_acc)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]

        print(f'Epoch {epoch + 1:3d}/{config.num_epochs} | '
              f'Time: {epoch_time:.2f}s | '
              f'LR: {current_lr:.4f} | '
              f'Train Loss: {train_loss / len(train_loader):.4f} | '
              f'Train Acc: {train_acc:.2f}% | '
              f'Test Acc: {test_acc:.2f}%')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'config': config.__dict__
            }, config.model_path)
            print(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {test_acc:.2f}%')

    print(f'\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%')
    print(f'ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {config.model_path}')


if __name__ == '__main__':
    train()